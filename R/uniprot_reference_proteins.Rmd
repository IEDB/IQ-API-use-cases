---
title: "IEDB Uniprot PDB entries"
author: "Jason Greenbaum"
date: "7/2/2021"
output: 
  html_document:
    code_folding: show
    df_print: paged
    toc: true
    toc_depth: 4
    theme: united
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
```

# IEDB Uniprot entries with associated PDB IDs

This use case came from a practical question of determining how many PDB records are associated with
each Uniprot record in the IEDB.  It is a basic demonstration of how we can query across systems
to get a meaningful  result.

## Setup

First, some housekeeping...

```{r}
library(httr)
library(jsonlite)
library(tidyr)
library(readr)
library(dplyr)
```


## Retrieve list of IEDB Uniprot IDs

Let's set the query string parameters.
```{r}
params <- list()
params[['parent_source_antigen_iri']] = 'like.UNIPROT*'
params[['select']] = 'parent_source_antigen_id,parent_source_antigen_iri,parent_source_antigen_source_org_iri,parent_source_antigen_name'
params[['offset']] = 0
params[['order']] = 'parent_source_antigen_id'
```

We define a function to query the API.  Since I know there will be more than 10K records, we will need to fetch 1 page at a time
and continue to append them onto our tibble.  **NOTE:** When paging through results by using an 'offset', it is critical to add an 'order' parameter to ensure that the pages are consistent between queries.

To be good citizens, we also add a short 'sleep' in between calls to the API.
```{r}

iq_query <- function(endpoint, 
                     query_params, 
                     base_uri='https://query-api.iedb.org/',
                     page_size = 10000) {

  # initialize 'get_text' so we can page through the results
  get_text <- 'NA'
  final_tbl <- tibble()
  url <- paste(base_uri, endpoint,sep='')
  
  # set the offset to 0
  offset <- 0
  
  # we must be careful not to use scientific notation with the offset parameter
  query_params[['offset']] <- format(offset, scientific = F)

  while(get_text != '[]') {
    print(paste0("fetching offset: ", query_params[['offset']]))
    #TODO: wrap this in a try block
    get_1 = GET(url, query=query_params)
    get_text = content(get_1,'text')
    resp_tbl <- tibble(fromJSON(get_text))
    final_tbl <- rbind(final_tbl, resp_tbl)
    offset <- offset + page_size
    query_params[['offset']] = format(offset, scientific = F)
    # sleep for 1 second between calls so as not to overload the server
    Sys.sleep(1)
  }

  # return the final_tbl
  final_tbl

}

full_tbl <- iq_query('antigen_search', params)
```


## Fetching PDB IDs associated with each Uniprot ID

Now we need to use the uniprot API to fetch PDB IDs associated with each entry.  First, we have to get the list of IDS. Since they're provided as IRIs, we need to remove the 'UNIPROT:' prefixes.

```{r}
full_tbl$uniprot_id <- gsub('UNIPROT:','',full_tbl$parent_source_antigen_iri)
uniprot_ids <- full_tbl$uniprot_id
```

Now we need to submit them in chunks of 500 to the Uniprot API.  We've commented out this section since it takes a while and instead, saved the PDB IDs in a file, which we load below.

```{r}
# pdb_tbl <- tibble()
# pdb_url = 'https://www.uniprot.org/uploadlists/'
# chunk_size = 500
# min_index = -chunk_size + 1
# max_index = 0
# while (max_index < length(uniprot_ids)) {
#   min_index = min_index + chunk_size
#   max_index = max_index + chunk_size
#   print(paste0("Mapping from ", min_index, " to ", max_index))
#   cur_ids = uniprot_ids[min_index:max_index]
#   cur_id_str = paste(cur_ids, collapse = ' ')
#   pdb_params = list(
#         'from' = 'ID',
#         'to'  = 'PDB_ID',
#         'format' = 'tab',
#         'query' = cur_id_str
#   )
#   get_p = GET(pdb_url, query=pdb_params)
#   get_p_text = content(get_p,'text')
#   resp_tbl <- read_tsv(get_p_text)
#   pdb_tbl <- rbind(pdb_tbl, resp_tbl)
#   Sys.sleep(1)
# }
# 
# # update the headers and save the pdb IDs as a file
#names(pdb_tbl) <- c('uniprot_id','pdb_id')
#write_tsv(pdb_tbl, 'pdb_ids.tsv')

```

Load the PDB IDs from a previously saved file:
```{r}
pdb_tbl <- read_tsv('pdb_ids.tsv')
```


## Merge the datasets and tally

```{r}
merged_tbl <- full_tbl %>%
  left_join(pdb_tbl,
            on='uniprot_id',
  )

collapsed_tbl <- merged_tbl %>%
  group_by(uniprot_id) %>%
  dplyr::summarize(pdb_ids=paste(pdb_id, collapse=','),
            n_pdbs=n()) %>%
  mutate(n_pdbs=ifelse(pdb_ids == 'NA',0,n_pdbs))

# now we can do an inner join to tie everything together
collapsed_tbl <- collapsed_tbl %>%
  inner_join(full_tbl,
             on='uniprot_id')

```

Now we have everything we want in 1 table.  Let's get some basic stats:

```{r}
collapsed_tbl %>%
  group_by(n_pdbs) %>%
  summarize(n_uniprot_ids=n())

```
This table shows how many PDB IDs are associated with IEDB Uniprot IDs. So, e.g., 67,685 Uniprot IDs are associated with 0 PDB IDs, 3,238 are associated with 1, and so on... 