{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IEDB Query API (IQ-API) - Use Case 1G\n",
    "**Goal**: Search for information related to a branch of the NCBI taxonomy, using Dengue virus and all Flaviviruses as examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this use case is to query for epitopes arising from a single branch of the NCBI taxonomy.  For example, extracting all viral epitopes or extracting all epitopes related to Dengue virus.  The approach outlined here can be applied to all tables where the *source_organism_iri_search* field exists.\n",
    "\n",
    "For more information on the expressive syntax of PostgresT, refer to [this document](https://postgrest.org/en/stable/api.html#).  For more details on the tables that are part of the API, refer to [the swagger documetation](http://query-api.iedb.org/docs/swagger/).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import required modules, set some globals, and define a function to print the corresponding CURL command for each request.  I've tried to include that CURL command for each example so that you can copy/paste it into your terminal.  You may want to pipe the output to a tool like 'jq' to have it render neatly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "base_uri='https://query-api.iedb.org'\n",
    "\n",
    "# funciton to print the CURL command given a request\n",
    "def print_curl_cmd(req):\n",
    "    url = req.url\n",
    "    print(\"curl -X 'GET' '\" + url + \"'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may or may not have resulted in a warning about lzma compression.  That can be safely ignored..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The IRI search fields\n",
    "\n",
    "Before we get started, we need to understand the fields that have 'iri_search' in their names.  For example, *source_organism_iri_search* or *host_organism_iri_search*.  All organism-related fields in the IEDB make use of the [NCBI Taxonomomy](https://www.ncbi.nlm.nih.gov/taxonomy) and reference the organism in question with its NCBI Taxonomy ID prefaced by the string 'NCBITaxon'.  For example, Dengue virus has the [NCBI Taxonomy ID 12637](https://www.ncbi.nlm.nih.gov/Taxonomy/Browser/wwwtax.cgi?mode=Info&id=12637) and would be referenced as 'NCBITaxon:12637'.  In the context of the NCBI Taxonomy, it's position looks like:\n",
    "\n",
    "```\n",
    "Organism (NCBITaxon:1)\n",
    "  Viruses (NCBITaxon:10239)\n",
    "    Riboviria (NCBITaxon:2559587)\n",
    "      Orthornavirae (NCBITaxon:2732396)\n",
    "        Kitrinoviricota (NCBITaxon:2732406)\n",
    "          Flasuviricetes (NCBITaxon:2732462)\n",
    "            Amarillovirales (NCBITaxon:2732545)\n",
    "              Flaviviridae (NCBITaxon:11050)\n",
    "                Flavivirus (NCBITaxon:11051)\n",
    "                  Dengue virus (NCBITaxon:12637)\n",
    "```\n",
    "\n",
    "In order to be able to make use of this hierarchy, the organsim taxonomy ID as well **many of its ancestor taxonomy IDs** are encoded into these *organism_iri_search* fields.  The *source_organism_iri_search* field of an epitope record from Dengue virus looks like:\n",
    "\n",
    "```json\n",
    "\"source_organism_iri_search\": [\n",
    "    \"NCBITaxon:1\",\n",
    "    \"NCBITaxon:10239\",\n",
    "    \"NCBITaxon:11050\",\n",
    "    \"NCBITaxon:11051\",\n",
    "    \"NCBITaxon:12637\",\n",
    "    \"OBI:0100026\"\n",
    "]\n",
    "```\n",
    "\n",
    "Note that there are several intermediate taxonomy IDs that are missing, skipping the taxonomy IDs in between Viruses (superkingdom) and Flaviviridae (family) levels.  You will also note an additional entry for [OBI:0100026](http://purl.obolibrary.org/obo/OBI_0100026), which corresponds to the 'organism' term in the [Ontology of Biomedical Investigations (OBI)](http://obi-ontology.org/).\n",
    "\n",
    "Other *iri_search* fields exist with values from taxonomies other than the NCBI.  For example:\n",
    "\n",
    "* *mhc_allele_iri_search* encodes values from the [MHC Restriction Ontology](http://www.obofoundry.org/ontology/mro.html)\n",
    "* *disease_iri_search* uses the [Human Disease Ontology](http://www.obofoundry.org/ontology/doid.html)\n",
    "\n",
    "The above list is not exhaustive.  For a detailed list of ontologies employed an the CURIEs used to refer to them, please consule the [curie_map](https://query-api.iedb.org/curie_map) table.\n",
    "\n",
    "\n",
    "## Query for Dengue epitopes\n",
    "\n",
    "So, in order to compose a query for all records below the 'Dengue virus' node in the taxonomy, all that is needed is to query on the *source_organism_iri_search* field for the presence of 'NCBITaxon:12637'.  Since this field is an array, we will need to use the [cs operator](https://postgrest.org/en/stable/api.html#operators) for 'contains'.  We also restrict the output to the small subset of fields that are relevant to our search with the [select operator](https://postgrest.org/en/stable/api.html#vertical-filtering-columns):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_params={ 'source_organism_iri_search': 'cs.{\"NCBITaxon:12637\"}',\n",
    "                'select': 'structure_iri,structure_description,curated_source_antigens,iedb_assay_ids,qualitative_measures,source_organism_names,source_organism_names,source_organism_iri_search',\n",
    "                'order': 'structure_iri',\n",
    "              }\n",
    "table_name='epitope_search'\n",
    "full_url=base_uri + '/' + table_name\n",
    "result = requests.get(full_url, params=search_params)\n",
    "print_curl_cmd(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK we have the result...now let's have a look.  **Note**: We are only printing the first record here to get a sense of what is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(result.json()[:1], indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also load the output into a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(result.json())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks as though exactly 10,000 records were returned.  This should raise suspicion as 10,000 is the default limit for a 'page' of results.  If you receive 10,000 results it indicates that you are only getting the first page back and you will need to pull the rest of the results in subsequent calls.  This is described further in the [IQ-API help material](https://help.iedb.org/hc/en-us/articles/4402872882189).\n",
    "\n",
    "### Retrieving more than 10,000 records\n",
    "\n",
    "We will need to retrieve the rest of the records by increasing the 'offset' parameter until we have pulled the complete dataset.  **WARNING:** Whenever requesting a result with multiple pages, **it is *essential* to add an 'order' keyword**.  Without it, the pages may be inconsistent between calls.\n",
    "\n",
    "We also add a pause of 2 seconds between calls in order to be a good citizen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_params['offset'] = 0\n",
    "while result.json() != []:\n",
    "    time.sleep(2)\n",
    "    search_params['offset'] += 10000\n",
    "    print('offset: ' + str(search_params['offset']))\n",
    "    result = requests.get(full_url, params=search_params)\n",
    "    df = df.append(pd.json_normalize(result.json()))\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take another look at our data frame, now that it's complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there are 10,004 Dengue epitope records, which matches what we obtain through the web interface as of August 27, 2021.\n",
    "\n",
    "## Query for Flavivirus epitopes\n",
    "\n",
    "What if we want to now search one level higher in the taxonomy, for epitopes from all of Flavivirus?  Simply update our query to use the NCBI Taxonomy ID of Flavivirus in the *source_organism_iri_search* field and repeat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_params={ 'source_organism_iri_search': 'cs.{\"NCBITaxon:11051\"}',\n",
    "                'select': 'structure_iri,structure_description,curated_source_antigens,iedb_assay_ids,qualitative_measures,source_organism_names,source_organism_names,source_organism_iri_search',\n",
    "                'offset': 0,\n",
    "                'order': 'structure_iri'\n",
    "              }\n",
    "table_name='epitope_search'\n",
    "full_url=base_uri + '/' + table_name\n",
    "\n",
    "# get the first 10K results and load them into a data frame\n",
    "result = requests.get(full_url, params=search_params)\n",
    "df_flavi = pd.json_normalize(result.json())\n",
    "\n",
    "while result.json() != []:\n",
    "    time.sleep(2)\n",
    "    search_params['offset'] += 10000\n",
    "    print('offset: ' + str(search_params['offset']))\n",
    "    result = requests.get(full_url, params=search_params)\n",
    "    df_flavi = df_flavi.append(pd.json_normalize(result.json()))\n",
    "print('Done!')\n",
    "\n",
    "df_flavi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK now we have 16,809 epitopes, which should include all of the Dengue epitopes.\n",
    "\n",
    "## Limiting the results to T cell epitopes\n",
    "\n",
    "What if we want to limit the output to peptides that were positive in at least one T cell assay (i.e., T cell epitopes).  We can perform essentiallly the same search against the **tcell_search** table, restricting it only to assays that do not have 'Negative' as their qualitative value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_params={ 'source_organism_iri_search': 'cs.{\"NCBITaxon:11051\"}',\n",
    "                'select': 'tcell_iri,structure_iri,linear_sequence,curated_source_antigen,qualitative_measure,assay_names,source_organism_name,source_organism_name',\n",
    "                'qualitative_measure': 'not.eq.Negative',\n",
    "                'offset': 0,\n",
    "                'order': 'tcell_iri'\n",
    "              }\n",
    "table_name='tcell_search'\n",
    "full_url=base_uri + '/' + table_name\n",
    "\n",
    "# get the first 10K results and load them into a data frame\n",
    "result = requests.get(full_url, params=search_params)\n",
    "df_tcell = pd.json_normalize(result.json())\n",
    "\n",
    "while result.json() != []:\n",
    "    time.sleep(2)\n",
    "    search_params['offset'] += 10000\n",
    "    print('offset: ' + str(search_params['offset']))\n",
    "    result = requests.get(full_url, params=search_params)\n",
    "    df_tcell = df_tcell.append(pd.json_normalize(result.json()))\n",
    "print('Done!')\n",
    "\n",
    "df_tcell\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10,217 positive T cell assays for Flavivirus.  That matches the web results for the IEDB as of August 27, 2021.  Note, however, that since we queried against the T cell table, the rows are unique by assay rather than epitope. So we will need to extract the unique epitopes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tcell.drop_duplicates(subset = ['linear_sequence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4,927 sequences from Flavivirus that were positive in at least 1 T cell assay, which matches the current data in the IEDB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An alternative approach using resource embeddings\n",
    "\n",
    "**NOTE:**  This example has not been fully vetted and takes a *very* long time to run, likely due to inefficient joins.  It will be further worked out in the future.\n",
    "\n",
    "We can also make use of [resource embeddings](https://postgrest.org/en/stable/api.html#resource-embedding) to get at this same information.  Here, we start with the epitope_search table, limit it to those with tcell_ids, then join it to the tcell_search table where the qualitative value is not 'Negative'.  Since results are returned for ALL epitope records with T cell assays, they need to be further processed to remove the epitopes without associated 'positive' T cell records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "search_params={ 'source_organism_iri_search': 'cs.{\"NCBITaxon:11051\"}',\n",
    "                'tcell_ids': 'not.is.null',\n",
    "                'tcell_search.qualitative_measure': 'not.eq.Negative',\n",
    "                'select': 'structure_id,tcell_ids,tcell_search(tcell_iri,qualitative_measure)',\n",
    "                'offset': 0,\n",
    "                'order': 'structure_id'\n",
    "              }\n",
    "table_name='epitope_search'\n",
    "full_url=base_uri + '/' + table_name\n",
    "\n",
    "# get the first 10K results and load them into a data frame\n",
    "result = requests.get(full_url, params=search_params)\n",
    "df_tcell_from_epitope = pd.json_normalize(result.json())\n",
    "\n",
    "while result.json() != []:\n",
    "    time.sleep(2)\n",
    "    search_params['offset'] += 10000\n",
    "    print('offset: ' + str(search_params['offset']))\n",
    "    result = requests.get(full_url, params=search_params)\n",
    "    df_tcell_from_epitope = df_tcell_from_epitope.append(pd.json_normalize(result.json()))\n",
    "print('Done!')\n",
    "\n",
    "df_tcell_from_epitope\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we filter out all of the rows where no corresponding, positive T cell assays were found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tcell_from_epitope[df_tcell_from_epitope['tcell_search'].apply(lambda x : len(x) > 0)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
